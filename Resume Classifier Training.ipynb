{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29818644-66ad-4c8b-ab3e-8fb3736e9752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|████████████████████████| 4760/4760 [00:00<00:00, 33282.30 examples/s]\n",
      "Map: 100%|████████████████████████| 1020/1020 [00:00<00:00, 27706.69 examples/s]\n",
      "Map: 100%|████████████████████████| 1020/1020 [00:00<00:00, 31428.16 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='894' max='894' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [894/894 03:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.884500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.225500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|████████████████████████| 4760/4760 [00:00<00:00, 32134.41 examples/s]\n",
      "Map: 100%|████████████████████████| 1020/1020 [00:00<00:00, 32650.71 examples/s]\n",
      "Map: 100%|████████████████████████| 1020/1020 [00:00<00:00, 34141.12 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='894' max='894' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [894/894 07:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.873600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.357000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: {'en_accuracy': 0.9833333333333333, 'ar_accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# train.py — Resume Classifier Training (AR + EN)\n",
    "\n",
    "#pip install -r requirements.txt\n",
    "\n",
    "\n",
    "import os, json, warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# ===== ثابتات بسيطة =====\n",
    "EN_CSV_PATH = \"/Users/reemabalharith/Desktop/Capston Project/english_cv_dataset_full_6800_enriched.csv\"   # عدّلي للمسار/الرابط\n",
    "AR_CSV_PATH = \"/Users/reemabalharith/Desktop/Capston Project/arabic_cv_dataset_full_6800_enriched.csv\"    # عدّلي للمسار/الرابط\n",
    "EN_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "AR_MODEL_NAME = \"aubmindlab/bert-base-arabertv02\"\n",
    "\n",
    "OUT_EN = \"llm_model_en\"\n",
    "OUT_AR = \"llm_model_ar\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "MAX_LENGTH = 256\n",
    "SEED = 42\n",
    "AR_FONT = \"Amiri-Regular.ttf\"\n",
    "\n",
    "# ===== أعمدة شائعة =====\n",
    "TEXT_CAND_EN = [\"Resume\",\"Cleaned_Resume\",\"summary\",\"Text\",\"text\"]\n",
    "TEXT_CAND_AR = TEXT_CAND_EN + [\"النص\",\"السيرة\"]\n",
    "LABEL_CAND_EN = [\"Category\",\"category\",\"Label\",\"label\"]\n",
    "LABEL_CAND_AR = LABEL_CAND_EN + [\"التصنيف\"]\n",
    "\n",
    "# ===== دوال مساعدة مختصرة =====\n",
    "def ar_shape(s: str) -> str:\n",
    "    if not isinstance(s, str): s = str(s)\n",
    "    return get_display(arabic_reshaper.reshape(s))\n",
    "\n",
    "def pick_col(df: pd.DataFrame, cands: List[str]) -> str:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    for c in cands:\n",
    "        if c.lower() in cols_lower: return cols_lower[c.lower()]\n",
    "    raise ValueError(f\"Missing any of {cands}\")\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# ===== الرسوم =====\n",
    "def plot_top7_and_pie(df: pd.DataFrame, label_col: str, out_prefix: str, lang: str, ar_font: Optional[str]=None):\n",
    "    counts = df[label_col].value_counts().head(7)\n",
    "    labels = counts.index.astype(str).tolist(); vals = counts.values\n",
    "    if lang=='ar' and ar_font and os.path.exists(ar_font):\n",
    "        matplotlib.rcParams['font.sans-serif'] = [ar_font]\n",
    "        labels_plot = [ar_shape(x) for x in labels]; title_bar = ar_shape('أكثر ٧ تصنيفات'); title_pie = ar_shape('أكثر ٧ تصنيفات (دائري)'); ylabel = ar_shape('العدد')\n",
    "    else:\n",
    "        labels_plot = labels; title_bar = 'Top-7 Categories'; title_pie = 'Top-7 Categories (Pie)'; ylabel = 'Count'\n",
    "    fig, ax = plt.subplots(figsize=(7,4)); ax.bar(labels_plot, vals); ax.set_title(title_bar); ax.set_ylabel(ylabel); ax.tick_params(axis='x', rotation=30); plt.tight_layout(); plt.savefig(f\"{out_prefix}_top7_bar.png\", dpi=150); plt.close(fig)\n",
    "    fig, ax = plt.subplots(figsize=(5,5)); ax.pie(vals, labels=labels_plot, autopct='%1.1f%%', startangle=140); ax.set_title(title_pie); plt.tight_layout(); plt.savefig(f\"{out_prefix}_top7_pie.png\", dpi=150); plt.close(fig)\n",
    "\n",
    "def plot_class_imbalance(df: pd.DataFrame, label_col: str, out_prefix: str, lang: str, ar_font: Optional[str]=None):\n",
    "    counts = df[label_col].value_counts(); labels = counts.index.astype(str).tolist()\n",
    "    if lang=='ar' and ar_font and os.path.exists(ar_font):\n",
    "        matplotlib.rcParams['font.sans-serif'] = [ar_font]\n",
    "        labels_plot = [ar_shape(x) for x in labels]; title = ar_shape('عدم توازن الفئات'); ylabel = ar_shape('العدد')\n",
    "    else:\n",
    "        labels_plot = labels; title = 'Class Imbalance'; ylabel = 'Count'\n",
    "    fig, ax = plt.subplots(figsize=(7,4)); ax.bar(labels_plot, counts.values); ax.set_title(title); ax.set_ylabel(ylabel); ax.tick_params(axis='x', rotation=30); plt.tight_layout(); plt.savefig(f\"{out_prefix}_class_imbalance.png\", dpi=150); plt.close(fig)\n",
    "\n",
    "def plot_wordcloud_en(df: pd.DataFrame, text_col: str, out_file: str):\n",
    "    text = ' '.join(map(str, df[text_col].dropna().tolist()))\n",
    "    wc = WordCloud(width=1000, height=500, background_color='white').generate(text)\n",
    "    fig, ax = plt.subplots(figsize=(10,5)); ax.imshow(wc); ax.axis('off'); ax.set_title('English Word Cloud'); plt.tight_layout(); plt.savefig(out_file, dpi=150); plt.close(fig)\n",
    "\n",
    "def plot_confusion(y_true: List[int], y_pred: List[int], id2label: Dict[int,str], out_file: str, lang: str, ar_font: Optional[str]=None):\n",
    "    labels_sorted = sorted(id2label.keys()); cm = confusion_matrix(y_true, y_pred, labels=labels_sorted); disp_labels = [id2label[i] for i in labels_sorted]\n",
    "    if lang=='ar' and ar_font and os.path.exists(ar_font): matplotlib.rcParams['font.sans-serif'] = [ar_font]; disp_labels = [ar_shape(x) for x in disp_labels]\n",
    "    fig, ax = plt.subplots(figsize=(6,6)); disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels); disp.plot(ax=ax, xticks_rotation=45, cmap='Blues', colorbar=False); ax.set_title('Confusion Matrix (AR)' if lang=='ar' else 'Confusion Matrix (EN)'); plt.tight_layout(); plt.savefig(out_file, dpi=160); plt.close(fig)\n",
    "\n",
    "def plot_accuracy_comparison(acc_map: Dict[str, float], out_file: str):\n",
    "    if not acc_map: return\n",
    "    names = list(acc_map.keys()); vals = [acc_map[n] for n in names]\n",
    "    fig, ax = plt.subplots(figsize=(6,4)); ax.bar(names, vals); ax.set_ylim(0,1.0); ax.set_ylabel('Accuracy'); ax.set_title('Model Accuracy Comparison'); ax.tick_params(axis='x', rotation=20); plt.tight_layout(); plt.savefig(out_file, dpi=150); plt.close(fig)\n",
    "\n",
    "# ===== تجهيز البيانات =====\n",
    "def factorize_labels(series: pd.Series) -> Tuple[np.ndarray, Dict[int,str]]:\n",
    "    labels, uniques = pd.factorize(series.astype(str), sort=True)\n",
    "    id2label = {int(i): str(lab) for i, lab in enumerate(uniques.tolist())}\n",
    "    return labels.astype(int), id2label\n",
    "\n",
    "def make_hf_dataset(texts: List[str], labels: List[int]) -> Dataset:\n",
    "    # استخدم المفتاح 'labels' (متوافق مع إصدارات قديمة/جديدة)\n",
    "    return Dataset.from_dict({\"text\": texts, \"labels\": labels})\n",
    "\n",
    "def tokenize_batch(batch, tokenizer, max_length: int):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# ===== تدريب لغة =====\n",
    "def train_one_language(csv_path: str, lang: str, model_name: str, output_dir: str, max_length=256, batch_size=16, epochs=3, lr=2e-5, seed=42, ar_font: Optional[str]=None) -> Dict[str,float]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    text_col = pick_col(df, TEXT_CAND_AR if lang=='ar' else TEXT_CAND_EN)\n",
    "    label_col = pick_col(df, LABEL_CAND_AR if lang=='ar' else LABEL_CAND_EN)\n",
    "    df = df[[text_col, label_col]].dropna(); df[text_col] = df[text_col].astype(str).str.strip(); df = df[df[text_col].str.len() > 0]\n",
    "\n",
    "    ensure_dir(output_dir); prefix = os.path.join(output_dir, f\"{lang}\")\n",
    "    plot_top7_and_pie(df, label_col, prefix, lang, ar_font)\n",
    "    plot_class_imbalance(df, label_col, prefix, lang, ar_font)\n",
    "    if lang=='en':\n",
    "        try: plot_wordcloud_en(df, text_col, os.path.join(output_dir, \"wordcloud_en.png\"))\n",
    "        except: pass\n",
    "\n",
    "    y, id2label = factorize_labels(df[label_col])\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(df[text_col].tolist(), y.tolist(), test_size=0.3, random_state=seed, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=seed, stratify=y_tmp)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label), id2label=id2label, label2id={v:k for k,v in id2label.items()})\n",
    "\n",
    "    ds_train = make_hf_dataset(X_train, y_train)\n",
    "    ds_val   = make_hf_dataset(X_val,   y_val)\n",
    "    ds_test  = make_hf_dataset(X_test,  y_test)\n",
    "\n",
    "    ds_train = ds_train.map(lambda b: tokenize_batch(b, tokenizer, max_length), batched=True)\n",
    "    ds_val   = ds_val.map(  lambda b: tokenize_batch(b, tokenizer, max_length), batched=True)\n",
    "    ds_test  = ds_test.map(  lambda b: tokenize_batch(b, tokenizer, max_length), batched=True)\n",
    "\n",
    "    # collator متوافق مع الإصدارات القديمة\n",
    "    def simple_collate(features):\n",
    "        return tokenizer.pad(features, return_tensors=\"pt\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(output_dir, \"hf_runs\"),\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=simple_collate,\n",
    "        compute_metrics=lambda p: {\n",
    "            \"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "            **{k:v for k,v in zip([\"precision\",\"recall\",\"f1\",\"_\"], precision_recall_fscore_support(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted', zero_division=0)) if k!=\"_\"}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_test = trainer.evaluate(ds_test)\n",
    "    metrics = {k: float(v) for k, v in eval_test.items() if isinstance(v, (int,float,np.floating))}\n",
    "\n",
    "    preds = np.argmax(trainer.predict(ds_test).predictions, axis=1)\n",
    "    plot_confusion(y_test, preds, id2label, os.path.join(output_dir, f\"cm_{lang}.png\"), lang, ar_font)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    with open(os.path.join(output_dir, \"labels.json\"), \"w\", encoding=\"utf-8\") as f: json.dump(id2label, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(output_dir, f\"metrics_{lang}.json\"), \"w\", encoding=\"utf-8\") as f: json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return {f\"{lang}_accuracy\": metrics.get('eval_accuracy', 0.0)}\n",
    "\n",
    "# ===== Main =====\n",
    "def main():\n",
    "    np.random.seed(SEED)\n",
    "    accs = {}\n",
    "    if EN_CSV_PATH:\n",
    "        ensure_dir(OUT_EN)\n",
    "        accs.update(train_one_language(EN_CSV_PATH, 'en', EN_MODEL_NAME, OUT_EN, MAX_LENGTH, BATCH_SIZE, EPOCHS, LR, SEED))\n",
    "    if AR_CSV_PATH:\n",
    "        ensure_dir(OUT_AR)\n",
    "        accs.update(train_one_language(AR_CSV_PATH, 'ar', AR_MODEL_NAME, OUT_AR, MAX_LENGTH, BATCH_SIZE, EPOCHS, LR, SEED, AR_FONT))\n",
    "    if accs:\n",
    "        plot_accuracy_comparison(accs, 'accuracy_comparison.png')\n",
    "        with open('accuracy_comparison.json','w',encoding='utf-8') as f: json.dump(accs, f, ensure_ascii=False, indent=2)\n",
    "        print(\"Accuracy:\", accs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e74d1-ce15-424f-b1b1-49364e74a91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee6d9a-f3d3-48f2-ab49-6db443056f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca785284-6eff-481c-b8d9-dc5b74af218a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
