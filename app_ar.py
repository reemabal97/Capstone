# app_ar.py â€” Ø¹Ø±Ø¨ÙŠ + RTL + PDF/TXT/DOCX + Debug + Normalization
import pdfplumber
import os, io, json, time, re
import pandas as pd
import numpy as np
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
# Ù…Ù„Ø§Ø­Ø¸Ø©: Ø³Ù†Ø­Ø§ÙˆÙ„ pypdf Ø£ÙˆÙ„Ù‹Ø§ØŒ ÙˆÙ„Ùˆ ÙØ´Ù„ Ù†Ø±Ø¬Ø¹ Ù„Ù€ pdfminer
try:
    from pypdf import PdfReader as PYPDFReader
    _HAS_PYPDF = True
except Exception:
    _HAS_PYPDF = False
from docx import Document
import matplotlib.pyplot as plt

# ==================== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© / Ø§Ù„Ø£Ù„ÙˆØ§Ù† ====================
st.set_page_config(page_title="Ù…ØµÙ†Ù‘Ù Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ© (Ø¹Ø±Ø¨ÙŠ)", page_icon="ğŸ§ ", layout="wide")
PRIMARY = "#10B981"  # emerald
ACCENT  = "#60A5FA"  # blue
DANGER  = "#EF4444"
BG      = "#0f172a"  # slate-900
CARD    = "#111827"  # slate-800
TEXT    = "#E5E7EB"  # slate-200

st.markdown(f"""
<style>
  .stApp {{
    direction: rtl;
    text-align: right;
    background: radial-gradient(1200px circle at 90% 0%, {BG} 0%, #030712 55%);
    color: {TEXT};
    font-family: "Cairo","Tajawal",system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Apple Color Emoji","Segoe UI Emoji";
  }}
  .glass {{
    background: linear-gradient(180deg, rgba(255,255,255,0.06), rgba(255,255,255,0.03));
    border: 1px solid rgba(255,255,255,0.08);
    border-radius: 18px;
    padding: 18px 20px;
    box-shadow: 0 20px 50px rgba(0,0,0,0.35);
  }}
  .pill {{ display:inline-block; padding:4px 10px; border-radius:999px; font-size:12px;
           border:1px solid rgba(255,255,255,.15); margin-left:6px; }}
  .primary {{ color:black; background:{PRIMARY}; border:none; }}
  .accent {{ color:black; background:{ACCENT}; border:none; }}
  h1, h2, h3, h4 {{ color:#F8FAFC; }}
  .small-muted {{ color:#94A3B8; font-size:13px; }}
</style>
""", unsafe_allow_html=True)

# ==================== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ====================
DEFAULT_MODEL_DIR = "llm_model_ar"
DEFAULT_TOPK = 5
DEFAULT_MAXLEN = 256
os.environ.setdefault("PYTORCH_MPS_HIGH_WATERMARK_RATIO", "0.0")

# ==================== ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ ====================
_ar_norm_map = str.maketrans({
    "Ø£":"Ø§","Ø¥":"Ø§","Ø¢":"Ø§","Ù‰":"ÙŠ","Ø¤":"Ùˆ","Ø¦":"ÙŠ","Ø©":"Ù‡",
})
_tashkeel = re.compile(r"[\u0617-\u061A\u064B-\u0652]")
def normalize_ar(text: str) -> str:
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙ + ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª
    text = _tashkeel.sub("", text)
    text = text.translate(_ar_norm_map)
    text = re.sub(r"[^\u0600-\u06FF0-9a-zA-Z\s%+@\-\.]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ==================== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ù„ØµÙ‚Ø§Øª ====================
@st.cache_resource(show_spinner=True)
def load_artifacts(model_dir: str):
    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
    mdl = AutoModelForSequenceClassification.from_pretrained(model_dir)
    labels_path = os.path.join(model_dir, "labels.json")
    if not os.path.exists(labels_path):
        raise FileNotFoundError(f"Ù„Ù… Ø£Ø¬Ø¯ labels.json Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯: {model_dir}")
    with open(labels_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # ÙŠØ¯Ø¹Ù… Ø´ÙƒÙ„ {"labels":[...]} Ø£Ùˆ {"id2label":{..},"label2id":{..}}
    if "labels" in data:
        labels = data["labels"]
        id2label = {i: lbl for i, lbl in enumerate(labels)}
    else:
        id2label = {int(k): v for k, v in data["id2label"].items()}
    # Ø«Ø¨ØªÙŠÙ‡Ø§ Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙˆÙ†ÙÙ‚ (Ù…Ù‡Ù… Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª)
    mdl.config.id2label = id2label
    mdl.config.label2id = {v: k for k, v in id2label.items()}
    mdl.eval()
    return tok, mdl, id2label

def pick_device():
    if torch.cuda.is_available(): return torch.device("cuda")
    if torch.backends.mps.is_available(): return torch.device("mps")
    return torch.device("cpu")

# ==================== Ù‚Ø±Ù‘Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª ====================
def read_txt(file) -> str:
    content = file.read()
    try:
        return content.decode("utf-8")
    except Exception:
        return content.decode("latin-1", errors="ignore")

def _pdf_text_with_pypdf(file) -> str:
    reader = PYPDFReader(file)
    pages = []
    for p in reader.pages:
        try:
            pages.append(p.extract_text() or "")
        except Exception:
            pages.append("")
    return "\n".join(pages)

def _pdf_text_with_pdfminer(file) -> str:
    # pdfminer ÙŠØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø± Ø£Ùˆ BytesIO
    from pdfminer.high_level import extract_text
    if hasattr(file, "read"):
        b = file.read()
        bio = io.BytesIO(b)
        txt = extract_text(bio)
    else:
        txt = extract_text(file)
    return txt

def _pdf_text_with_pdfplumber(file) -> str:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ PDF Ø¨Ø¯Ù‚Ø© Ø£ÙØ¶Ù„ Ù„Ù„Ø¹Ø±Ø¨ÙŠ ÙˆØ¨Ø¯ÙˆÙ† OCR.
    ÙŠÙ‚Ø¨Ù„ UploadedFile Ø£Ùˆ bytes.
    """
    # Ø®Ø°ÙŠ Ø§Ù„Ø¨Ø§ÙŠØªØ§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹
    if hasattr(file, "read"):
        b = file.read()
        if hasattr(file, "seek"): 
            file.seek(0)  # Ù†Ø±Ø¬Ù‘Ø¹ Ø§Ù„Ù…Ø¤Ø´Ø± Ù„Ø£Ù†Ù†Ø§ Ù‚Ø¯ Ù†Ù‚Ø±Ø£Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§
    else:
        b = file  # bytes

    txt_pages = []
    with pdfplumber.open(io.BytesIO(b)) as pdf:
        for page in pdf.pages:
            t = page.extract_text() or ""
            txt_pages.append(t)

    out = "\n".join(txt_pages)
    out = re.sub(r"\s+", " ", out).strip()
    return out

def extract_text_ar(uploaded) -> str:
    """
    Ù†Ø­Ø§ÙˆÙ„ pypdf Ø£ÙˆÙ„Ù‹Ø§Ø› Ù„Ùˆ Ø³ÙŠÙ‘Ø¦ Ù†Ø¬Ø±Ø¨ pdfplumberØ› Ù„Ùˆ Ù…Ø§ Ù†ÙØ¹ Ù†Ø¬Ø±Ø¨ pdfminer.
    """
    txt = ""

    # 1) pypdf Ø£ÙˆÙ„Ù‹Ø§ (Ù„Ùˆ Ù…ØªÙˆÙØ±)
    try:
        if _HAS_PYPDF:
            txt = _pdf_text_with_pypdf(uploaded)
        else:
            txt = ""
    except Exception:
        txt = ""

    # Ù…Ø¹ÙŠØ§Ø± Ø¨Ø³ÙŠØ· Ù„Ù„Ø­ÙƒÙ… Ø¹Ù„Ù‰ Ø³ÙˆØ¡ Ø§Ù„Ù†Øµ
    def is_bad(s: str) -> bool:
        s = (s or "").strip()
        return (len(s) < 120) or ("ï»¼" in s) or (s.count("\u200f") > 50)

    # 2) Ù„Ùˆ Ø³ÙŠÙ‘Ø¦ â†’ pdfplumber
    if is_bad(txt):
        try:
            if hasattr(uploaded, "seek"): uploaded.seek(0)
            txt2 = _pdf_text_with_pdfplumber(uploaded)
            if len(txt2) > len(txt): 
                txt = txt2
        except Exception:
            pass

    # 3) Ù„Ùˆ Ù…Ø§ Ø²Ø§Ù„ Ø³ÙŠÙ‘Ø¦ â†’ pdfminer (Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø¯Ùƒ Ø³Ø§Ø¨Ù‚Ù‹Ø§)
    if is_bad(txt):
        try:
            if hasattr(uploaded, "seek"): uploaded.seek(0)
            txt3 = _pdf_text_with_pdfminer(uploaded)
            if len(txt3) > len(txt): 
                txt = txt3
        except Exception:
            pass

    return re.sub(r"\s+", " ", txt or "").strip()


def read_docx(file) -> str:
    file_bytes = io.BytesIO(file.read())
    doc = Document(file_bytes)
    return "\n".join([p.text for p in doc.paragraphs]).strip()

def load_file_text(uploaded) -> str:
    name = uploaded.name.lower()
    if name.endswith(".pdf"):
        # Ù†Ø­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø¤Ø´Ø± Ù„Ø£Ù† Ø§Ù„Ø¯ÙˆØ§Ù„ Ù‚Ø¯ ØªÙ‚Ø±Ø£Ù‡ Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø±Ù‘Ø©
        if hasattr(uploaded, "seek"): uploaded.seek(0)
        return extract_text_ar(uploaded)
    if name.endswith(".txt"):   return read_txt(uploaded)
    if name.endswith(".docx"):  return read_docx(uploaded)
    raise ValueError("ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©. Ø§Ø±ÙØ¹ÙŠ PDF Ø£Ùˆ TXT Ø£Ùˆ DOCX.")

# ==================== Ø§Ù„ØªÙ†Ø¨Ø¤ ====================
def predict_text(raw_text: str, tok, mdl, id2label, device, max_len=DEFAULT_MAXLEN, top_k=DEFAULT_TOPK):
    # ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆÙƒÙ†Ù†Ø©
    text = normalize_ar(raw_text)
    enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=max_len)
    enc = {k: v.to(device) for k, v in enc.items()}
    with torch.no_grad():
        logits = mdl(**enc).logits
        probs = torch.softmax(logits, dim=-1).squeeze(0)
        k = min(top_k, probs.shape[-1])
        conf, idx = torch.topk(probs, k=k)
    items = [(id2label[i.item()], float(c)) for i, c in zip(idx, conf)]
    return items, text  # Ù†Ø±Ø¬Ø¹ Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ù„Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„ØªØ´Ø®ÙŠØµÙŠØ©


def bar_row(value, label, color=PRIMARY):
    st.markdown(
        f"""<div class="glass" style="background:{CARD}; margin:6px 0;">
              <div style="display:flex; justify-content:space-between; align-items:center;">
                <div style="font-weight:600;">{label}</div>
                <div style="font-weight:700;">{value:.1f}%</div>
              </div>
              <div style="height:10px; background:#1f2937; border-radius:999px; overflow:hidden; margin-top:6px;">
                <div style="height:10px; width:{value}%; background:{color};"></div>
              </div>
            </div>""",
        unsafe_allow_html=True
    )

def df_topk(items):
    return pd.DataFrame([{"Ø§Ù„ÙØ¦Ø©": lbl, "Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„": round(p, 6)} for lbl, p in items])

# ==================== Ø±Ø£Ø³ Ø§Ù„ØµÙØ­Ø© ====================
st.markdown(
    f"""
<div class="glass" style="padding:24px; margin-bottom:14px;">
  <div style="display:flex; gap:10px; align-items:center; flex-wrap:wrap;">
    <div class="pill primary">Ù†Ù…ÙˆØ°Ø¬ LLM</div>
    <div class="pill">PDF</div>
    <div class="pill">TXT</div>
    <div class="pill">DOCX</div>
    <div class="pill accent">CSV Ø¯ÙØ¹ÙŠ</div>
  </div>
  <h1 style="margin:6px 0 0 0;">ğŸ§  Ù…ØµÙ†Ù‘Ù Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ© â€” Ø¹Ø±Ø¨ÙŠ</h1>
  <div class="small-muted">Ø§Ø±ÙØ¹ÙŠ <b>PDF / TXT / DOCX</b> Ø£Ùˆ Ø§Ù„ØµÙ‚ÙŠ Ø§Ù„Ù†Øµ Ù…Ø¨Ø§Ø´Ø±Ø©. ÙŠØ¯Ø¹Ù… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¯ÙØ¹ÙŠ Ø¹Ø¨Ø± CSV.</div>
  <div class="small-muted">Ø¨ÙˆØ§Ø³Ø·Ø©: Ø±ÙŠÙ…Ø§ Ø¨Ø§Ù„Ø­Ø§Ø±Ø«</div>
</div>
""",
    unsafe_allow_html=True
)

# ==================== Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ====================
with st.sidebar:
    st.markdown("### âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    model_dir = st.text_input("Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬", value=DEFAULT_MODEL_DIR,
                              help="Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ÙˆÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­ÙˆÙ‘Ù„ ÙˆÙ…Ù„Ù labels.json")
    max_len = st.slider("Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ (ØªÙˆÙƒÙ†Ø²)", 64, 512, DEFAULT_MAXLEN, step=32)
    top_k   = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Top-k)", 1, 10, DEFAULT_TOPK)
    show_bars = st.checkbox("Ø¥Ø¸Ù‡Ø§Ø± Ø£Ø´Ø±Ø·Ø© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª", value=True)
    device = pick_device()
    st.success(f"Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {device}")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
try:
    tok, mdl, id2label = load_artifacts(model_dir)
    mdl.to(device)
except Exception as e:
    st.error(f"ØªØ¹Ø°Ù‘Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† **{model_dir}**\n\n{e}")
    st.stop()

# ==================== Ø§Ù„ØªØ¨ÙˆÙŠØ¨Ø§Øª ====================
t1, t2, t3 = st.tabs(["ğŸ”® ØªÙ†Ø¨Ø¤", "ğŸ“¦ Ø¯ÙØ¹ÙŠ (CSV)", "ğŸ“Š Ø§Ø³ØªÙƒØ´Ø§Ù (EDA)"])

# -------- ØªØ¨ÙˆÙŠØ¨ Ø§Ù„ØªÙ†Ø¨Ø¤ --------
with t1:
    colA, colB = st.columns([1,1])

    with colA:
        st.subheader("Ù„ØµÙ‚ Ù†Øµ")
        txt = st.text_area("Ø§Ù„ØµÙ‚ÙŠ Ù†Øµ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© Ù‡Ù†Ø§:", height=220, placeholder="Ø§ÙƒØªØ¨ÙŠ Ø£Ùˆ Ø§Ù„ØµÙ‚ÙŠ Ø§Ù„Ù†Øµâ€¦")
        btn_txt = st.button("ØªÙ†Ø¨Ø¤ Ù…Ù† Ø§Ù„Ù†Øµ", type="primary", use_container_width=True)

    with colB:
        st.subheader("Ø±ÙØ¹ Ù…Ù„Ù (PDF / TXT / DOCX)")
        file = st.file_uploader("Ø§Ø®ØªØ§Ø±ÙŠ Ù…Ù„ÙÙ‹Ø§", type=["pdf", "txt", "docx"], accept_multiple_files=False)
        btn_file = st.button("ØªÙ†Ø¨Ø¤ Ù…Ù† Ø§Ù„Ù…Ù„Ù", use_container_width=True)

    def run_predict_ar(raw_text: str, *, show_debug_block: bool = True):
        if not raw_text or not raw_text.strip():
            st.warning("ÙØ¶Ù„Ø§Ù‹ Ø§Ø¯Ø®Ù„ÙŠ Ù†ØµÙ‹Ø§ Ø£Ùˆ Ø§Ø±ÙØ¹ÙŠ Ù…Ù„ÙÙ‹Ø§.")
            return
        # Ù†Ø¹Ø±Ø¶ Ù…Ù‚ØªØ·Ù Ù‚Ø¨Ù„ ÙˆÙ…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        if show_debug_block:
            with st.expander("ğŸ” Ù…Ù‚ØªØ·Ù Ù…Ù† Ø§Ù„Ù†Øµ Ù‚Ø¨Ù„ Ø§Ù„ØªØµÙ†ÙŠÙ (Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø³Ù„Ø§Ù…ØªÙ‡)", expanded=True):
                st.code((raw_text or "")[:500], language="text")
        with st.spinner("Ø¬Ø§Ø±Ù Ø§Ù„ØªÙ†Ø¨Ø¤â€¦"):
            items, norm_text = predict_text(raw_text, tok, mdl, id2label, device, max_len=max_len, top_k=top_k)
        # ØªØ­Ø°ÙŠØ± Ù„Ùˆ Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        if len((raw_text or "").strip()) < 120:
            st.warning("Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§ â€” Ù‚Ø¯ ØªÙƒÙˆÙ† Ù†ØªÙŠØ¬Ø© PDF ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©. Ø¬Ø±Ù‘Ø¨ÙŠ Ø±ÙØ¹ Ù†Ø³Ø®Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… pdfminer.")
        pred, conf = items[0]
        st.markdown(f"### âœ… Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: **{pred}**  Â·  Ø§Ù„ÙˆØ«ÙˆÙ‚ÙŠØ©: **{conf:.1%}**")
        if show_bars:
            for i, (lbl, p) in enumerate(items):
                bar_row(p*100, lbl, color=ACCENT if i==0 else PRIMARY)
        st.markdown("#### Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Top-k)")
        st.dataframe(df_topk(items), use_container_width=True)

    if btn_txt:
        run_predict_ar(txt)

    if btn_file:
        if file is None:
            st.warning("ÙØ¶Ù„Ø§Ù‹ Ø§Ø®ØªØ§Ø±ÙŠ Ù…Ù„ÙÙ‹Ø§ Ø£ÙˆÙ„Ø§Ù‹.")
        else:
            try:
                raw_text = load_file_text(file)
                st.caption(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ **{file.name}** â€” Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù: {len(raw_text)}")
                run_predict_ar(raw_text)
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")

# -------- ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø¯ÙØ¹ÙŠ --------
with t2:
    st.subheader("ØªÙ†Ø¨Ø¤ Ø¯ÙØ¹ÙŠ Ù…Ù† CSV")
    st.caption("ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ CSV Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø§Ø³Ù…Ù‡ **text** (Ù†Øµ Ø§Ù„Ø³ÙŠØ±Ø©).")
    csv = st.file_uploader("Ø§Ø±ÙØ¹ CSV", type=["csv"], accept_multiple_files=False, key="csvup_ar")
    if csv is not None:
        try:
            df = pd.read_csv(csv)
        except Exception as e:
            st.error(f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© CSV: {e}")
            st.stop()
        if "text" not in df.columns:
            st.error("ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ CSV Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø¨Ø§Ø³Ù… `text`.")
        else:
            df["text"] = df["text"].fillna("").astype(str)
            preds, topk_json = [], []
            prog = st.progress(0, text="ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øªâ€¦")
            t0 = time.time()
            for i, t in enumerate(df["text"].tolist(), start=1):
                items, _ = predict_text(t, tok, mdl, id2label, device, max_len=max_len, top_k=top_k)
                preds.append(items[0][0])
                topk_json.append(json.dumps([{"label":l,"prob":float(p)} for l,p in items], ensure_ascii=False))
                prog.progress(i/len(df))
            dt = time.time() - t0

            out = df.copy()
            out["prediction"] = preds
            out["topk_probs"] = topk_json

            st.success(f"ØªÙ…: {len(df)} ØµÙÙ‹Ø§ Â· {dt:.1f}Ø«")
            st.dataframe(out.head(25), use_container_width=True)

            st.download_button(
                "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬",
                out.to_csv(index=False).encode("utf-8"),
                file_name="predictions_ar.csv",
                mime="text/csv",
                use_container_width=True
            )

# -------- ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù --------
with t3:
    st.subheader("Ø§Ø³ØªÙƒØ´Ø§Ù Ø³Ø±ÙŠØ¹ (Ø­Ø³Ø¨ ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)")
    st.caption("Ø§Ø±ÙØ¹ÙŠ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª Ø£Ùˆ Ø§Ù„ØµÙ‚ÙŠ Ø¹Ø¯Ù‘Ø© Ù†ØµÙˆØµ Ù…ÙØµÙˆÙ„Ø© Ø¨Ù€ --- Ù„Ø±Ø¤ÙŠØ© ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©.")
    files = st.file_uploader("Ø§Ø±ÙØ¹ÙŠ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª (PDF/TXT/DOCX)", type=["pdf","txt","docx"], accept_multiple_files=True, key="edafiles_ar")
    bulk_txt = st.text_area("Ø£Ùˆ Ø§Ù„ØµÙ‚ÙŠ Ø¹Ø¯Ø© Ø³ÙŠØ± (Ø§ÙØµÙ„ÙŠ Ø¨ÙŠÙ†Ù‡Ø§ Ø¨Ù€ ---)", height=160, key="edatxt_ar",
                            placeholder="Ø³ÙŠØ±Ø© 1 â€¦\n---\nØ³ÙŠØ±Ø© 2 â€¦\n---\nØ³ÙŠØ±Ø© 3 â€¦")
    go = st.button("ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù", type="primary")

    if go:
        texts = []
        if bulk_txt.strip():
            parts = [p.strip() for p in bulk_txt.split("---") if p.strip()]
            texts.extend(parts)
        for f in files or []:
            try:
                texts.append(load_file_text(f))
            except Exception as e:
                st.warning(f"ØªØ®Ø·ÙŠ {getattr(f,'name','file')}: {e}")

        if not texts:
            st.warning("ÙØ¶Ù„Ø§Ù‹ Ø§Ø¯Ø®Ù„ÙŠ Ù†ØµÙˆØµÙ‹Ø§ Ø£Ùˆ Ø§Ø±ÙØ¹ÙŠ Ù…Ù„ÙØ§Øª Ø£ÙˆÙ„Ø§Ù‹.")
        else:
            labels_pred = []
            for t in texts:
                items, _ = predict_text(t, tok, mdl, id2label, device, max_len=DEFAULT_MAXLEN, top_k=DEFAULT_TOPK)
                labels_pred.append(items[0][0])

            dist = pd.Series(labels_pred).value_counts().sort_values(ascending=False)
            st.markdown("#### ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©")
            fig, ax = plt.subplots(figsize=(8,4))
            dist.plot(kind="bar", ax=ax, color="#38bdf8")
            ax.set_xlabel("Ø§Ù„ÙØ¦Ø©"); ax.set_ylabel("Ø§Ù„Ø¹Ø¯Ø¯"); ax.set_title("ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª")
            st.pyplot(fig, clear_figure=True)

            st.markdown("#### Ø¬Ø¯ÙˆÙ„")
            st.dataframe(dist.reset_index().rename(columns={"index":"Ø§Ù„ÙØ¦Ø©", 0:"Ø§Ù„Ø¹Ø¯Ø¯"}), use_container_width=True)

# ==================== ØªØ°ÙŠÙŠÙ„ ====================
st.markdown(
    f"""
<div class="small-muted" style="margin-top:24px; text-align:center;">
  ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯ <b>{DEFAULT_MODEL_DIR}</b> ÙƒÙ…ØµØ¯Ø± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ).<br/>
  Ø§Ù„Ø±ÙØ¹ ÙŠØ¯Ø¹Ù… <b>PDF</b> Ùˆ<b>TXT</b> Ùˆ<b>DOCX</b> Ù†ØµÙ‹Ø§ ØµØ±ÙŠØ­Ù‹Ø§ ÙƒÙ…Ø§ Ø·ÙÙ„ÙØ¨. Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¯ÙØ¹ÙŠ ÙŠØªÙˆÙ‚Ø¹ Ø¹Ù…ÙˆØ¯ <code>text</code>.
</div>
""",
    unsafe_allow_html=True
)
